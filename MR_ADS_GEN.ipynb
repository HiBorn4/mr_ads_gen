{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yZba3JXb_EVy",
        "outputId": "2b2e8b8c-adba-4fa8-f40e-b0800a0b363e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hi_born/anaconda3/envs/mr_ads_gen/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# # Install necessary libraries\n",
        "# %pip install torch torchvision\n",
        "# %pip install transformers\n",
        "# %pip install pillow\n",
        "# %pip install ftfy regex tqdm\n",
        "# %pip install git+https://github.com/openai/CLIP.git\n",
        "# %pip install pytorch-fid\n",
        "# %pip install torch-fidelity\n",
        "# %pip install pandas\n",
        "# %pip install matplotlib\n",
        "# %pip install numpy\n",
        "# %pip install scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.utils import save_image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pytorch_fid import fid_score\n",
        "from torch_fidelity import calculate_metrics\n",
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kWbBah3I_aLz"
      },
      "outputs": [],
      "source": [
        "ad_dataset_path = 'AD_Dataset_with_ClassLabel_1.csv'\n",
        "user_dataset_path = 'User_Dataset.csv'\n",
        "images_folder_path = 'image_embeddings'\n",
        "\n",
        "\n",
        "# Load CSV files\n",
        "ad_dataset = pd.read_csv(ad_dataset_path)\n",
        "user_dataset = pd.read_csv(user_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v4yx3M1CwJ1",
        "outputId": "7737c284-40a1-4d79-f265-3b78fae352f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 100.00% (563/563 batches)\n",
            "All embeddings loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "ad_dataset_path = 'AD_Dataset_with_ClassLabel_1.csv'\n",
        "user_dataset_path = 'User_Dataset.csv'\n",
        "image_embedding_folder = 'image_embeddings'\n",
        "text_embedding_folder = 'text_embeddings'\n",
        "\n",
        "# Load datasets\n",
        "ad_dataset = pd.read_csv(ad_dataset_path)\n",
        "user_dataset = pd.read_csv(user_dataset_path)\n",
        "\n",
        "def find_text_embedding_path(ad_id):\n",
        "    # Construct the expected file name based on the ad_id (adjust if necessary)\n",
        "    embedding_filename = f\"text_embedding_{int(ad_id)}.npy\"  # Assuming this naming convention\n",
        "    embedding_path = os.path.join(text_embedding_folder, embedding_filename)\n",
        "    \n",
        "    # Check if the file exists, if so return the path, else return None or a placeholder\n",
        "    if os.path.exists(embedding_path):\n",
        "        return embedding_path\n",
        "    else:\n",
        "        return None  # Or raise an error, or use a placeholder string\n",
        "\n",
        "\n",
        "ad_dataset['text_embedding_path'] = ad_dataset['ad_id'].apply(find_text_embedding_path)\n",
        "\n",
        "# Verify paths and dataset structure\n",
        "print(ad_dataset[['ad_id', 'text_embedding_path']].head())\n",
        "print(f\"Image embedding folder contains: {len(os.listdir(image_embedding_folder))} files.\")\n",
        "\n",
        "# Function to load text embeddings from .npy files with error handling\n",
        "def load_text_embedding(filepath):\n",
        "    try:\n",
        "        if os.path.exists(filepath):\n",
        "            return np.load(filepath)\n",
        "        else:\n",
        "            print(f\"File not found: {filepath}\")\n",
        "            return None\n",
        "    except EOFError:\n",
        "        print(f\"EOFError: Unable to load {filepath} â€“ File might be empty or corrupted.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error while loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load text embeddings into the dataset with error handling\n",
        "ad_dataset['text_embedding'] = ad_dataset['text_embedding_path'].apply(lambda path: load_text_embedding(path))\n",
        "\n",
        "# Display a few loaded text embeddings for verification\n",
        "print(f\"Sample text embeddings:\\n{ad_dataset['text_embedding'].head()}\")\n",
        "\n",
        "\n",
        "# Function to load image embeddings from .pt files\n",
        "def load_image_embedding(filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        return torch.load(filepath, map_location=device)\n",
        "    else:\n",
        "        print(f\"Image embedding not found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "# Load text embeddings into the dataset\n",
        "ad_dataset['text_embedding'] = ad_dataset['text_embedding_path'].apply(lambda path: load_text_embedding(path))\n",
        "\n",
        "# Example: Display some loaded text embeddings\n",
        "print(f\"Loaded text embedding sample:\\n{ad_dataset['text_embedding'].head()}\")\n",
        "\n",
        "# Function to construct the correct image embedding path using saved Drive structure\n",
        "def find_image_embedding_file(image_id):\n",
        "    embedding_filename = f\"embedding_{image_id}.pt\"\n",
        "    embedding_path = os.path.join(image_embedding_folder, embedding_filename)\n",
        "\n",
        "    if os.path.exists(embedding_path):\n",
        "        return embedding_path\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Image embedding not found for ID: {image_id}\")\n",
        "\n",
        "# Function to load a single image embedding from .pt file\n",
        "def load_image_embedding(filepath):\n",
        "    try:\n",
        "        if os.path.exists(filepath):\n",
        "            return torch.load(filepath, map_location=device)\n",
        "        else:\n",
        "            print(f\"Image embedding not found: {filepath}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embedding from {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to process image embeddings in batches\n",
        "def load_image_embeddings(dataset, batch_size=16):\n",
        "    num_batches = math.ceil(len(dataset) / batch_size)\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        batch_data = dataset.iloc[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
        "        batch_tensors = []\n",
        "\n",
        "        # Load each embedding in the batch\n",
        "        for _, row in batch_data.iterrows():\n",
        "            try:\n",
        "                embedding_path = find_image_embedding_file(row['id'])  # Adjusted for correct path resolution\n",
        "                img_embedding = load_image_embedding(embedding_path)\n",
        "                if img_embedding is not None:\n",
        "                    batch_tensors.append(img_embedding)\n",
        "            except FileNotFoundError as e:\n",
        "                print(e)\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error: {e}\")\n",
        "\n",
        "        if batch_tensors:\n",
        "            # Stack tensors and move to GPU (if available)\n",
        "            batch_tensors = torch.stack(batch_tensors).to(device)\n",
        "\n",
        "            # Save the batch tensor to disk\n",
        "            save_path = os.path.join(image_embedding_folder, f\"loaded_batch_{batch_idx}.pt\")\n",
        "            torch.save(batch_tensors.cpu(), save_path)  # Save on CPU to free GPU memory\n",
        "            print(f\"Saved batch {batch_idx} embeddings to {save_path}\")\n",
        "\n",
        "            # Clear GPU memory after processing each batch\n",
        "            del batch_tensors\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Display progress\n",
        "        clear_output(wait=True)\n",
        "        progress = (batch_idx + 1) / num_batches * 100\n",
        "        print(f\"Progress: {progress:.2f}% ({batch_idx + 1}/{num_batches} batches)\")\n",
        "\n",
        "# Start loading image embeddings\n",
        "load_image_embeddings(ad_dataset, batch_size=16)\n",
        "\n",
        "print(\"All embeddings loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv5HEJyK-aRI",
        "outputId": "5d961918-2159-44f2-dffd-81b9f0e5d48b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['ad_id', 'image_path', 'text', 'dimensions', 'Click-through(CTR)',\n",
              "       'Cost per Click(CPC)', 'Return on Ad Spend(ROAS)',\n",
              "       'Conversion(Value less than CTR)', 'Purchase_Conversion_rate',\n",
              "       'ClassLabel_final', 'text_embedding_path', 'text_embedding'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ad_dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-j4ETTnBAcS",
        "outputId": "60a498a6-8306-42d1-ab12-81f20611bce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of embeddings: 9000\n"
          ]
        }
      ],
      "source": [
        "# Check the total number of embeddings\n",
        "total_embeddings = len(ad_dataset['text_embedding'])\n",
        "print(f\"Total number of embeddings: {total_embeddings}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1INWDpSA1e0",
        "outputId": "41e5e51a-6ae2-4ff6-c9a0-a025c2fcb1dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indices with embedding size 768: []\n"
          ]
        }
      ],
      "source": [
        "# Find the indices where text_embedding is not None and size is 768\n",
        "correct_size_indices = ad_dataset[ad_dataset['text_embedding'].apply(lambda x: x is not None and len(x) == 768)].index\n",
        "print(f\"Indices with embedding size 768: {list(correct_size_indices)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HTivobfCF9x",
        "outputId": "932543f0-c97b-42d8-9a5d-7172f8d410a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    [[0.06809492, -0.12058817, 0.23107693, 0.02658...\n",
            "1    [[0.10373393, -0.10898319, 0.5905778, 0.213023...\n",
            "2    [[0.045375224, -0.4151973, 0.37503538, 0.05655...\n",
            "3    [[-0.2609169, -0.20043637, 0.32284147, 0.13952...\n",
            "4    [[0.07046847, -0.31986868, 0.058868058, 0.2812...\n",
            "Name: text_embedding, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Check the first few rows of the text_embedding column\n",
        "print(ad_dataset['text_embedding'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYgtqqVCCRUa",
        "outputId": "995fe2c0-5385-40f5-f51b-a9ccd58a63cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    768\n",
            "1    768\n",
            "2    768\n",
            "3    768\n",
            "4    768\n",
            "Name: text_embedding, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Flatten each embedding in the text_embedding column\n",
        "ad_dataset['text_embedding'] = ad_dataset['text_embedding'].apply(lambda x: np.array(x).flatten())\n",
        "\n",
        "# Now check the length of the flattened embeddings\n",
        "embedding_lengths = ad_dataset['text_embedding'].apply(len)\n",
        "print(embedding_lengths.head())  # To verify the sizes after flattening\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2t5Zi2UCkNa",
        "outputId": "c3cea3d7-493d-4eeb-faa5-de4b35c8d713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text_embedding\n",
            "768    8999\n",
            "1         1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check the length of each embedding and identify any that are not size 768\n",
        "embedding_lengths = ad_dataset['text_embedding'].apply(lambda x: len(x) if x is not None else 0)\n",
        "print(embedding_lengths.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kLOjNvVDZSz",
        "outputId": "25b9517e-ab7e-41f6-fb8e-9020420abbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text_embedding\n",
            "768    9000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Replace embeddings of size 1 with zero vectors of size 768\n",
        "ad_dataset['text_embedding'] = ad_dataset['text_embedding'].apply(lambda x: x if len(x) == 768 else np.zeros(768))\n",
        "\n",
        "# Confirm that all embeddings are now of size 768\n",
        "embedding_lengths = ad_dataset['text_embedding'].apply(len)\n",
        "print(embedding_lengths.value_counts())  # This should only show 768 now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpBJ1jfAPSjU",
        "outputId": "0f88fb70-0e57-4891-bc7b-2050cc6c33d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the scaler for numerical features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = 250  # Adjust as needed\n",
        "\n",
        "# Extract numerical features\n",
        "numerical_features = ['Conversion(Value less than CTR)',\n",
        "                      'Cost per Click(CPC)',\n",
        "                      'Return on Ad Spend(ROAS)']\n",
        "\n",
        "# Fit the scaler on the entire numerical data\n",
        "scaler.fit(ad_dataset[numerical_features])\n",
        "\n",
        "def batch_generator(dataset, batch_size):\n",
        "    \"\"\"Generates batches of data to avoid memory overload.\"\"\"\n",
        "    num_samples = len(dataset)\n",
        "    for start_idx in range(0, num_samples, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, num_samples)\n",
        "        yield dataset.iloc[start_idx:end_idx]\n",
        "\n",
        "def process_batch(batch):\n",
        "    \"\"\"Processes a single batch by normalizing numerical features and combining them.\"\"\"\n",
        "    # Extract and normalize numerical features, ensuring consistent DataFrame input\n",
        "    numerical_data = scaler.transform(batch[numerical_features])\n",
        "\n",
        "    # Extract categorical features\n",
        "    categorical_features = batch.drop(\n",
        "        columns=numerical_features + ['text', 'image_path', 'text_embedding', 'ad_id']\n",
        "    )\n",
        "    categorical_data = categorical_features.values\n",
        "\n",
        "    # Extract text embeddings\n",
        "    text_embeddings = np.vstack(batch['text_embedding'])\n",
        "\n",
        "    # Combine all features for this batch\n",
        "    return np.concatenate([numerical_data, text_embeddings, categorical_data], axis=1)\n",
        "\n",
        "# Initialize a list to store the combined feature vectors\n",
        "feature_vectors_list = []\n",
        "\n",
        "# Process the dataset in batches\n",
        "for batch in batch_generator(ad_dataset, BATCH_SIZE):\n",
        "    batch_features = process_batch(batch)\n",
        "    feature_vectors_list.append(batch_features)\n",
        "\n",
        "# Concatenate all batch results into the final feature vector array\n",
        "feature_vectors = np.vstack(feature_vectors_list)\n",
        "\n",
        "print(\"Feature extraction completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1Kk9Hcu_A5j6"
      },
      "outputs": [],
      "source": [
        "# Merge ad_dataset with user_dataset on 'ad_id'\n",
        "merged_dataset = pd.merge(ad_dataset, user_dataset, on='ad_id', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LXJTDL-zCjd5"
      },
      "outputs": [],
      "source": [
        "# One-Hot Encode 'gender' and 'location'\n",
        "merged_dataset = pd.get_dummies(merged_dataset, columns=['gender', 'location'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfF0V9NRQKuf",
        "outputId": "817ff3b3-5700-4341-bb13-59b26e804d7f"
      },
      "outputs": [],
      "source": [
        "# Clone the StyleGAN2 repository\n",
        "# %git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "# %cd stylegan2-ada-pytorch\n",
        "# %pip install click\n",
        "\n",
        "# Import StyleGAN modules\n",
        "import sys\n",
        "sys.path.append('/content/stylegan2-ada-pytorch')\n",
        "import dnnlib\n",
        "import legacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "vwxrufUlvfYF",
        "outputId": "6040747f-ea1e-4b0a-f9c5-556ab85bb681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Prepare feature vectors as PyTorch tensors (Assume `feature_vectors` is defined)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m feature_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Vectors Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_tensors\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Assume `batch_tensors` is loaded earlier into memory\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import dnnlib\n",
        "import legacy  # Part of StyleGAN2-ADA repository\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Prepare feature vectors as PyTorch tensors (Assume `feature_vectors` is defined)\n",
        "feature_tensors = torch.tensor(feature_vectors, dtype=torch.float32).to(device)\n",
        "print(f\"Feature Vectors Shape: {feature_tensors.shape}\")\n",
        "\n",
        "# Assume `batch_tensors` is loaded earlier into memory\n",
        "print(f\"Loaded Batch Tensors Shape: {batch_tensors.shape}\")\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "BATCH_SIZE = 64  # Adjust based on your GPU capacity\n",
        "dataset = TensorDataset(batch_tensors, feature_tensors)\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Load pretrained StyleGAN2-ADA Generator and Discriminator\n",
        "with dnnlib.util.open_url('https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl') as f:\n",
        "    pretrained_network = legacy.load_network_pkl(f)\n",
        "    G = pretrained_network['G_ema'].to(device)  # Generator\n",
        "    D = pretrained_network['D'].to(device)      # Discriminator\n",
        "\n",
        "# Instantiate the Conditioning Network\n",
        "class ConditioningNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, conditioning_dim):\n",
        "        super(ConditioningNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, conditioning_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "conditioning_dim = 512\n",
        "input_dim = feature_vectors.shape[1]\n",
        "conditioning_net = ConditioningNetwork(input_dim, conditioning_dim).to(device)\n",
        "\n",
        "# Initialize optimizers\n",
        "optimizer_G = torch.optim.Adam(conditioning_net.parameters(), lr=0.002, betas=(0.0, 0.99))\n",
        "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.002, betas=(0.0, 0.99))\n",
        "\n",
        "# Initialize mixed precision scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Define checkpoint saving function\n",
        "def save_checkpoint(epoch, G, D, optimizer_G, optimizer_D, scaler):\n",
        "    os.makedirs('./checkpoints', exist_ok=True)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'G_state_dict': G.state_dict(),\n",
        "        'D_state_dict': D.state_dict(),\n",
        "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "    }, f'./checkpoints/checkpoint_epoch_{epoch}.pth')\n",
        "\n",
        "# Training Loop with Mixed Precision\n",
        "num_epochs = 10  # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Print statement to track the epoch progress\n",
        "    print(f\"Starting Epoch [{epoch+1}/{num_epochs}]...\")\n",
        "\n",
        "    for i, (real_images, features) in enumerate(data_loader):\n",
        "        real_images, features = real_images.to(device), features.to(device)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            # Generate conditioning inputs\n",
        "            conditioning_inputs = conditioning_net(features)\n",
        "\n",
        "            # Generate latent codes\n",
        "            latent_codes = torch.randn(real_images.size(0), G.z_dim, device=device)\n",
        "\n",
        "            # Generate fake images\n",
        "            fake_images = G(latent_codes, c=conditioning_inputs)\n",
        "\n",
        "            # Discriminator loss\n",
        "            D_real = D(real_images, conditioning_inputs)\n",
        "            D_fake = D(fake_images.detach(), conditioning_inputs)\n",
        "            loss_D = -torch.mean(D_real) + torch.mean(D_fake)\n",
        "\n",
        "            # Generator loss\n",
        "            D_fake_G = D(fake_images, conditioning_inputs)\n",
        "            loss_G = -torch.mean(D_fake_G)\n",
        "\n",
        "        # Backpropagate discriminator loss\n",
        "        scaler.scale(loss_D).backward()\n",
        "        scaler.step(optimizer_D)\n",
        "\n",
        "        # Backpropagate generator loss\n",
        "        scaler.scale(loss_G).backward()\n",
        "        scaler.step(optimizer_G)\n",
        "\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(data_loader)}], \"\n",
        "                  f\"Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
        "\n",
        "    # Save checkpoint at the end of each epoch\n",
        "    save_checkpoint(epoch, G, D, optimizer_G, optimizer_D, scaler)\n",
        "\n",
        "    # Print statement to indicate epoch completion\n",
        "    print(f\"Completed Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "print(\"Training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DVZiynjWpF5",
        "outputId": "ba31d5c6-1b8b-4f68-daec-edf6c4d85e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Vectors Data Type: <class 'numpy.ndarray'>\n",
            "Feature Vectors Sample: [[-0.5755673978965404 -1.2161115807481238 0.9976056249796139 ... 2\n",
            "  'comic_book' 'text_embeddings/text_embedding_1.npy']\n",
            " [2.465561307929546 -1.4631324780353672 -1.057857036482435 ... 3\n",
            "  'web_site' 'text_embeddings/text_embedding_2.npy']\n",
            " [-0.7276238331878448 -0.6136689469523845 -0.35559445568784986 ... 1\n",
            "  'web_site' 'text_embeddings/text_embedding_3.npy']\n",
            " [-0.7276238331878448 -0.5409317818424095 -1.075097259826736 ... 1\n",
            "  'organ' 'text_embeddings/text_embedding_4.npy']\n",
            " [-0.7276238331878448 -0.38909401341167027 -0.5942288447759985 ... 0\n",
            "  'web_site' 'text_embeddings/text_embedding_5.npy']]\n"
          ]
        }
      ],
      "source": [
        "# Inspect the contents of feature_vectors\n",
        "print(f\"Feature Vectors Data Type: {type(feature_vectors)}\")\n",
        "print(f\"Feature Vectors Sample: {feature_vectors[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "igxLr0QDtYT2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Feature Vectors Dimensions: 9000, 776\n",
            "Cleaned Feature Vectors Dimensions: (9000, 776)\n",
            "Image Tensors Dimensions: torch.Size([9000, 3, 256, 256])\n",
            "Batch 1 - Real Images: torch.Size([8, 3, 256, 256]), Features: torch.Size([8, 776])\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'conditioning_net' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Real Images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreal_images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Generate conditioning inputs\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m conditioning_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mconditioning_net\u001b[49m(features)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Generate latent codes\u001b[39;00m\n\u001b[1;32m     69\u001b[0m latent_codes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, G\u001b[38;5;241m.\u001b[39mz_dim)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conditioning_net' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 8  # Adjust based on your memory constraints\n",
        "num_epochs = 10  # Number of epochs for training\n",
        "\n",
        "def clean_feature_vectors(feature_vectors):\n",
        "    \"\"\"Convert all elements to floats, handling tuples, strings, and invalid entries.\"\"\"\n",
        "    cleaned_vectors = []\n",
        "    for vector in feature_vectors:\n",
        "        cleaned_vector = []\n",
        "        for item in vector:\n",
        "            if isinstance(item, str):\n",
        "                try:\n",
        "                    # Convert string that looks like a tuple, e.g., '(300, 250)'\n",
        "                    item = eval(item)\n",
        "                    if isinstance(item, tuple):\n",
        "                        # Take the average of tuple values\n",
        "                        item = sum(item) / len(item)\n",
        "                except:\n",
        "                    item = 0.0  # Default to 0.0 if conversion fails\n",
        "\n",
        "            # Ensure item is a valid numeric type\n",
        "            if isinstance(item, (int, float)):\n",
        "                cleaned_vector.append(float(item))\n",
        "            else:\n",
        "                cleaned_vector.append(0.0)  # Default to 0.0 for invalid types\n",
        "\n",
        "        cleaned_vectors.append(cleaned_vector)\n",
        "\n",
        "    return np.array(cleaned_vectors, dtype=np.float32)\n",
        "\n",
        "# Clean the feature vectors and ensure they are properly formatted\n",
        "print(f\"Original Feature Vectors Dimensions: {len(feature_vectors)}, {len(feature_vectors[0])}\")\n",
        "feature_vectors_cleaned = clean_feature_vectors(feature_vectors)\n",
        "\n",
        "# Print cleaned feature vectors dimensions for verification\n",
        "print(f\"Cleaned Feature Vectors Dimensions: {feature_vectors_cleaned.shape}\")\n",
        "\n",
        "# Convert cleaned feature vectors to PyTorch tensors (CPU only)\n",
        "feature_tensors = torch.tensor(feature_vectors_cleaned, dtype=torch.float32)\n",
        "\n",
        "# Generate placeholder image tensors (adjust shape if needed)\n",
        "image_tensors = torch.zeros(len(feature_tensors), 3, 256, 256, dtype=torch.float32)\n",
        "\n",
        "# Print the dimensions of the image tensors\n",
        "print(f\"Image Tensors Dimensions: {image_tensors.shape}\")\n",
        "\n",
        "# Create DataLoader for efficient batch processing\n",
        "dataset = TensorDataset(image_tensors, feature_tensors)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, features) in enumerate(data_loader):\n",
        "        # Ensure tensors are on CPU\n",
        "        real_images = real_images.to('cpu', non_blocking=True)\n",
        "        features = features.to('cpu', non_blocking=True)\n",
        "\n",
        "        # Print batch dimensions (for debugging)\n",
        "        print(f\"Batch {i + 1} - Real Images: {real_images.shape}, Features: {features.shape}\")\n",
        "\n",
        "        # Generate conditioning inputs\n",
        "        conditioning_inputs = conditioning_net(features)\n",
        "\n",
        "        # Generate latent codes\n",
        "        latent_codes = torch.randn(batch_size, G.z_dim)\n",
        "\n",
        "        # Generate fake images with conditioning\n",
        "        fake_images = G(latent_codes, c=conditioning_inputs)\n",
        "\n",
        "        # Train Discriminator\n",
        "        D_real = D(real_images, conditioning_inputs)\n",
        "        D_fake = D(fake_images.detach(), conditioning_inputs)\n",
        "        loss_D = -torch.mean(D_real) + torch.mean(D_fake)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        D_fake = D(fake_images, conditioning_inputs)\n",
        "        loss_G = -torch.mean(D_fake)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
